{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"soc-redditHyperlinks-body.tsv\"\n",
    "# testing the first 10 rows\n",
    "df = pd.read_csv(file_path, sep='\\t').head(10000)\n",
    "\n",
    "G = nx.from_pandas_edgelist(df, 'SOURCE_SUBREDDIT', 'TARGET_SUBREDDIT', create_using=nx.DiGraph())\n",
    "\n",
    "# Centrality Measures\n",
    "degree_centrality = centrality.degree_centrality(G)\n",
    "closeness_centrality = centrality.closeness_centrality(G)\n",
    "# Print Centrality Measures\n",
    "for node in G.nodes():\n",
    "    print(f\"Node: {node}, Degree Centrality: {degree_centrality[node]}, Closeness Centrality: {closeness_centrality[node]}\")\n",
    "\n",
    "# Influence Analysis\n",
    "pagerank_scores = nx.pagerank(G)\n",
    "influential_subreddits = sorted(pagerank_scores, key=pagerank_scores.get, reverse=True)\n",
    "# Print top influential subreddits\n",
    "print(\"Top Influential Subreddits:\")\n",
    "for subreddit in influential_subreddits[:10]:\n",
    "    print(subreddit, pagerank_scores[subreddit])\n",
    "\n",
    "# Perform community detection using the Louvain algorithm\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "# Print the number of communities and subreddits in each community\n",
    "print(f\"Number of communities: {len(communities)}\")\n",
    "for i, community in enumerate(communities):\n",
    "    print(f\"Community {i + 1}: {len(community)} subreddits\")\n",
    "# Visualize the communities\n",
    "pos = nx.spring_layout(G)  # use a different layout algorithm based on preference\n",
    "colors = [i for i, comm in enumerate(communities) for _ in comm]\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G, pos, node_color=colors, with_labels=True, cmap=plt.cm.tab20, font_size=8)\n",
    "plt.title(\"Subreddit Community Detection\")\n",
    "plt.show()\n",
    "\n",
    "# Set edge weights to 1 for each edge\n",
    "edge_weights = {(source, target): 1 for source, target in G.edges()}\n",
    "# Visualize the flow of information through edge thickness\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, font_size=8, node_color='skyblue', node_size=1000,\n",
    "        edgelist=edge_weights.keys(), width=list(edge_weights.values()), edge_color='gray', edge_cmap=plt.cm.Blues)\n",
    "plt.title(\"Content Propagation Analysis\")\n",
    "plt.show()\n",
    "\n",
    "# ============ CONTENT ANALYSIS=================\n",
    "# Convert 'POST_ID' to numeric\n",
    "df['POST_ID'] = pd.to_numeric(df['POST_ID'], errors='coerce')\n",
    "# Drop rows with NaN values in 'POST_ID'\n",
    "df = df.dropna(subset=['POST_ID'])\n",
    "# Assuming you have a column named 'POST_ID' in your dataframe\n",
    "top_posts = df.nlargest(1000, 'POST_ID')\n",
    "# Display the content of the top 5 posts\n",
    "for index, row in top_posts.iterrows():\n",
    "    print(f\"Post ID: {row['POST_ID']}\")\n",
    "    print(f\"Source Subreddit: {row['SOURCE_SUBREDDIT']}\")\n",
    "    print(f\"Target Subreddit: {row['TARGET_SUBREDDIT']}\")\n",
    "    print(f\"Timestamp: {row['TIMESTAMP']}\")\n",
    "    print(f\"Link Sentiment: {row['LINK_SENTIMENT']}\")\n",
    "    print(f\"Properties: {row['PROPERTIES']}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
